## 1. Worker Node
- each Node has multiple Pods on it 
- Below 3 process must be installed on every node
- 1. Container Runtime
- 2. Kubelet : It interacts with both-the container and node. It is responsible for taking configuration and running pod with cointainer inside and assigning resources from Node to container(eg. CPU, RAM, ROM)
- 3. Kube proxy : 
   - Kube proxy forwards the requests.
   - its job is to look for new services and every time new services created, it creates a appropriates the rules on each node to forward traffic to those services to backend pods.
   - it make sure low network overhead eg. If my-app on node1 want to access database, it will not forward that request randomly to any node instead it will forward the request to database(pod) on same node

 ![image](https://user-images.githubusercontent.com/74223025/229471186-abe8077b-2ec8-4c34-9094-f3dff8f66a47.png)
 
## 2. Master Node
<img src="https://user-images.githubusercontent.com/74223025/229474124-a4403681-47cf-4f17-ad85-7b971307eaf1.png" width="600" height="300">

Below are the 4 process that runs on every master node.
- 1. <b>API server</b> 
   - Whenever you want to deploy new application in k8 cluster you interacts with API server using some clients(UI,kuberntes dashboard, kubelet, kubernetes API). 
   - It is like cluster gateway
   - It also acts as a gatekeeper for authentication, to make sure only authentic request will get pass through
   - It means whenever you schedule new pods, deploy new applications and create new service, you have to interact with API server.
   - Good for security beacuase only one entry point into cluster.

&emsp; &emsp; &emsp; <tb><tb><img src="https://user-images.githubusercontent.com/74223025/229493503-10d239a4-9f3d-4a24-b629-e7d9d46b2c34.png" width="600" height="300">

- 2. <b>Scheduler</b>
   - Scheduler just decides on which Node new pod should be scheduled, but actual process is executed by kubelet on that node. 
   - When you will reqeust API server to create new pod, it will validate the reqeust and pass it to Scheduler, which will decide on which node this pod should be Schedule.
   - Scheduler will first check our request that how much resources needed for and then it will check each nodes for available resources. 
   - Further it will Schedule new pod on node where maximum resources are available. 
    ![image](https://user-images.githubusercontent.com/74223025/229505751-c968b3c6-c1b5-4029-96f1-9d927de15ca4.png)

- 3. <b>Controller Manager</b>
   - detects the cluster state changes
   - When pods died on any node, <b>Controller Manager</b> will detect it and reschedule it.
   - Further scheduler check resources on each pod and will schedule it on avaialable node
    ![image](https://user-images.githubusercontent.com/74223025/229512732-b9748518-a03c-4080-ac93-6c0ce394b987.png)


- 4. ETCD
   - It is a distrubuted reliable key-value store that is simple, secure and fast.
   - It's store the information regarding a cluster such as Nodes, pods, Configs, Secrets, Accounts, Roles, Bindings, Others
   - Every information you see after running a "Kubectl get" command is from the ETCD server
   - Every channe is updated in ETCD server and unless it is not considered as changed
   - It does not stores data of applications like databases.
   ![image](https://user-images.githubusercontent.com/74223025/230017267-814f4fed-c8d4-451b-a584-04904cc1f68e.png)

  
  
![image](https://user-images.githubusercontent.com/74223025/230018331-11cf274e-5d89-4303-9bcd-ddd19d1b056c.png)
